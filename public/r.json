{
    "name": "project",
    "type": "registry:block",
    "dependencies": [
      "@genkit-ai/googleai",
      "@genkit-ai/next",
      "@hookform/resolvers",
      "@mediapipe/tasks-vision",
      "@tanstack-query-firebase/react",
      "@tanstack/react-query",
      "@tensorflow/tfjs-core",
      "date-fns",
      "dotenv",
      "firebase",
      "genkit",
      "lucide-react",
      "patch-package",
      "react-day-picker",
      "react-hook-form",
      "react-markdown",
      "recharts",
      "tailwindcss-animate",
      "zod"
    ],
    "devDependencies": [
      "genkit-cli",
      "postcss"
    ],
    "registryDependencies": [
      "accordion",
      "alert",
      "alert-dialog",
      "avatar",
      "badge",
      "button",
      "calendar",
      "card",
      "chart",
      "checkbox",
      "dialog",
      "dropdown-menu",
      "form",
      "input",
      "label",
      "menubar",
      "popover",
      "progress",
      "radio-group",
      "scroll-area",
      "select",
      "separator",
      "sheet",
      "sidebar",
      "skeleton",
      "slider",
      "switch",
      "table",
      "tabs",
      "textarea",
      "tooltip"
    ],
    "files": [
      {
        "path": "ai/dev.ts",
        "content": "import { config } from 'dotenv';\nconfig();\n\nimport '@/ai/flows/generate-interview-questions.ts';\nimport '@/ai/flows/transcribe-answer.ts';\nimport '@/ai/flows/parse-resume.ts';\nimport '@/ai/flows/evaluate-answer.ts';\nimport '@/ai/flows/analyze-video-performance.ts';\n",
        "type": "registry:component",
        "target": "ai/dev.ts"
      },
      {
        "path": "ai/flows/analyze-video-performance.ts",
        "content": "'use server';\n/**\n * @fileOverview Analyzes user's facial expression data for performance cues.\n *\n * - analyzeVideoPerformance - A function that handles the video performance analysis.\n * - AnalyzeVideoPerformanceInput - The input type for the analyzeVideoPerformance function.\n * - AnalyzeVideoPerformanceOutput - The return type for the analyzeVideoPerformance function.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst AnalyzeVideoPerformanceInputSchema = z.object({\n  facialDataJson: z\n    .string()\n    .describe(\n      \"A JSON string representing an array of facial detection data collected over time. Each element is an object with 'blendshapes' (an array from MediaPipe's FaceLandmarker), or null if no face was detected.\"\n    ),\n});\nexport type AnalyzeVideoPerformanceInput = z.infer<typeof AnalyzeVideoPerformanceInputSchema>;\n\nconst AnalyzeVideoPerformanceOutputSchema = z.object({\n  nervousnessAnalysis: z.string().describe(\"A textual analysis of the user's nervousness, based on their facial blendshapes over time (e.g., fluctuations, prevalence of surprise or fear-related blendshapes).\"),\n  confidenceScore: z.number().min(0).max(10).describe(\"A numerical score from 0 to 10 for the user's confidence. Base this on sustained neutral or smile-related blendshapes.\"),\n  gazeAnalysis: z.string().describe(\"An analysis of the user's focus. Since we don't have head pose, infer this from the consistency of face detection. Mention that gaze wasn't tracked directly.\"),\n  cheatingSuspicion: z.boolean().describe(\"A flag indicating potential cheating. Since we cannot track gaze from this data, this should generally be false unless expression data is completely absent for long periods, suggesting the user was not in front of the camera. \"),\n});\nexport type AnalyzeVideoPerformanceOutput = z.infer<typeof AnalyzeVideoPerformanceOutputSchema>;\n\nexport async function analyzeVideoPerformance(input: AnalyzeVideoPerformanceInput): Promise<AnalyzeVideoPerformanceOutput> {\n  return analyzeVideoPerformanceFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'analyzeVideoPerformancePrompt',\n  input: {schema: AnalyzeVideoPerformanceInputSchema},\n  output: {schema: AnalyzeVideoPerformanceOutputSchema},\n  prompt: `You are an expert interview coach who specializes in analyzing non-verbal communication. Analyze the provided time-series data of a candidate's facial blendshapes recorded during an interview answer.\n\n  The input is a JSON string representing an array of snapshots. Each snapshot is either null (no face detected) or an object containing a 'blendshapes' array from MediaPipe FaceLandmarker. Each item in the blendshapes array has a 'categoryName' and a 'score' (0-1).\n\n  Facial Blendshape Data Log:\n  {{{facialDataJson}}}\n\n  Based on this data, provide the following analysis:\n  1.  **Nervousness Analysis**: Evaluate the stability of blendshapes. Frequent, high-magnitude fluctuations might indicate nervousness. High scores for blendshapes like 'jawOpen', 'mouthPout', 'browDownLeft', 'browDownRight', or 'eyeWidener' could suggest nervousness or surprise. A consistent, stable set of neutral or positive blendshapes suggests calmness. Provide a brief textual summary.\n  2.  **Confidence Score**: On a scale of 0 to 10, how confident does the candidate appear? High confidence can be inferred from sustained high scores for smile-related blendshapes ('mouthSmileLeft', 'mouthSmileRight', 'cheekSquintLeft'). Low confidence might be indicated by high scores in frowning or worried blendshapes ('mouthFrownLeft', 'mouthFrownRight', 'browInnerUp').\n  3.  **Gaze Analysis**: The data does not include head pose or eye tracking. State that direct gaze analysis is not possible. You can make a general comment on focus based on whether a face was detected consistently. For example, if many entries are null, it might suggest the user was not consistently in front of the camera.\n  4.  **Cheating Suspicion**: Set the 'cheatingSuspicion' flag to 'false'. Since we cannot track eye movement away from the screen with the given data, we cannot reliably detect cheating. Only set it to true if there are large gaps in the data (many null entries) where no face was detected, which could imply the user left the camera's view.\n\n  Return your complete analysis in the specified JSON format.`,\n});\n\nconst analyzeVideoPerformanceFlow = ai.defineFlow(\n  {\n    name: 'analyzeVideoPerformanceFlow',\n    inputSchema: AnalyzeVideoPerformanceInputSchema,\n    outputSchema: AnalyzeVideoPerformanceOutputSchema,\n  },\n  async input => {\n    const {output} = await prompt(input);\n    return output!;\n  }\n);\n",
        "type": "registry:component",
        "target": "ai/flows/analyze-video-performance.ts"
      },
      {
        "path": "ai/flows/evaluate-answer.ts",
        "content": "// ai/flows/evaluate-answer.ts\n'use server';\n/**\n * @fileOverview Evaluates user answers to interview questions, providing feedback, a score, follow-up questions, expected answer elements, and suggested resources.\n *\n * - evaluateAnswer - A function that evaluates the answer.\n * - EvaluateAnswerInput - The input type for the evaluateAnswer function.\n * - EvaluateAnswerOutput - The return type for the evaluateAnswer function.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst EvaluateAnswerInputSchema = z.object({\n  question: z.string().describe('The interview question asked.'),\n  answer: z.string().describe('The answer provided by the user.'),\n  resumeData: z.string().describe('The extracted resume data in JSON format.'),\n});\nexport type EvaluateAnswerInput = z.infer<typeof EvaluateAnswerInputSchema>;\n\nconst FollowUpQuestionSuggestionSchema = z.object({\n  followUpQuestion: z.string().describe('A suggested follow-up question.'),\n});\n\nconst SuggestedResourceSchema = z.object({\n  title: z.string().describe('The title of the suggested resource.'),\n  url: z.string().describe('The URL of the suggested resource (e.g., an article or documentation).'),\n});\n\nconst EvaluateAnswerOutputSchema = z.object({\n  evaluation: z.string().describe('The textual evaluation of the answer.'),\n  score: z.number().min(0).max(10).describe('A numerical score from 0 to 10 for the answer. 0 is poor, 10 is excellent.'),\n  followUpQuestion: z.string().describe('A suggested follow-up question.'),\n  expectedAnswerElements: z.string().describe('A summary of key points or elements the AI was expecting in an ideal answer.'),\n  suggestedResources: z.array(SuggestedResourceSchema).describe('An array of 1-2 suggested resources (e.g., articles, documentation) to enhance understanding, each with a title and URL.'),\n});\nexport type EvaluateAnswerOutput = z.infer<typeof EvaluateAnswerOutputSchema>;\n\nconst followUpQuestionSuggestionTool = ai.defineTool({\n  name: 'followUpQuestionSuggestion',\n  description: 'Suggests a relevant follow-up question based on the answer and the resume.',\n  inputSchema: z.object({\n    question: z.string().describe('The original interview question.'),\n    answer: z.string().describe('The user provided answer.'),\n    resumeData: z.string().describe('The resume data to base the follow up question on.'),\n  }),\n  outputSchema: FollowUpQuestionSuggestionSchema,\n  async run(input) {\n    // Just return the follow up question. The LLM will handle the\n    // tool calling and formatting of the response.\n    return {\n      followUpQuestion: `Based on the question (\"${input.question}\"), the answer (\"${input.answer}\"), and the resume (\"${input.resumeData}\"), what is a good follow-up question to ask?`,\n    };\n  },\n});\n\nconst evaluateAnswerPrompt = ai.definePrompt({\n  name: 'evaluateAnswerPrompt',\n  tools: [followUpQuestionSuggestionTool],\n  input: {schema: EvaluateAnswerInputSchema},\n  output: {schema: EvaluateAnswerOutputSchema},\n  prompt: `You are an expert interview evaluator. Please evaluate the candidate's answer to the question, taking into account their resume data.\n\n    Question: {{{question}}}\n    Answer: {{{answer}}}\n    Resume Data: {{{resumeData}}}\n\n    Consider the following:\n    - Did the candidate answer the question directly?\n    - Did the candidate provide sufficient detail?\n    - Did the candidate use specific examples to support their answer?\n    - How does the answer relate to the information provided in their resume?\n\n    Provide a textual evaluation of the answer.\n    Also, provide a numerical score from 0 to 10 for the answer, where 0 is poor and 10 is excellent.\n    Describe the key elements or points you were expecting in an ideal answer for this specific question.\n    Suggest 1-2 relevant online resources (e.g., articles, documentation links) that would help the candidate deepen their understanding of the question's topic. For each resource, provide a title and a URL.\n\n    In addition to your evaluation, score, expected answer elements, and suggested resources, use the followUpQuestionSuggestion tool to suggest ONE follow-up question that would help you better assess the candidate's skills and experience. The tool's description is: Suggests a relevant follow-up question based on the answer and the resume.\n\n    Return the evaluation, the score, the expected answer elements, the suggested resources, and the follow up question.\n    `,\n});\n\nconst evaluateAnswerFlow = ai.defineFlow(\n  {\n    name: 'evaluateAnswerFlow',\n    inputSchema: EvaluateAnswerInputSchema,\n    outputSchema: EvaluateAnswerOutputSchema,\n  },\n  async input => {\n    const {output} = await evaluateAnswerPrompt(input);\n    return output!;\n  }\n);\n\nexport async function evaluateAnswer(input: EvaluateAnswerInput): Promise<EvaluateAnswerOutput> {\n  return evaluateAnswerFlow(input);\n}\n\nexport type {FollowUpQuestionSuggestionSchema, SuggestedResourceSchema};\n",
        "type": "registry:component",
        "target": "ai/flows/evaluate-answer.ts"
      },
      {
        "path": "ai/flows/generate-interview-questions.ts",
        "content": "// ai/flows/generate-interview-questions.ts\n'use server';\n\n/**\n * @fileOverview Generates personalized interview questions based on the provided resume data and desired quantity.\n *\n * - generateInterviewQuestions - A function that generates interview questions.\n * - GenerateInterviewQuestionsInput - The input type for the generateInterviewQuestions function.\n * - GenerateInterviewQuestionsOutput - The return type for the generateInterviewQuestions function.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst GenerateInterviewQuestionsInputSchema = z.object({\n  resumeData: z\n    .string()\n    .describe('The extracted data from the resume, including work experience, skills, and projects.'),\n  numberOfQuestions: z.number().optional().default(5).describe('The desired number of interview questions. Defaults to 5.'),\n});\nexport type GenerateInterviewQuestionsInput = z.infer<typeof GenerateInterviewQuestionsInputSchema>;\n\nconst QuestionObjectSchema = z.object({\n  question: z.string().describe('The interview question text.'),\n  guidanceLink: z.string().describe('A URL (e.g., Google search) to help understand the question.'),\n});\n\nconst GenerateInterviewQuestionsOutputSchema = z.object({\n  questions: z.array(QuestionObjectSchema).describe('An array of personalized interview questions, each with text and a guidance link.'),\n});\nexport type GenerateInterviewQuestionsOutput = z.infer<typeof GenerateInterviewQuestionsOutputSchema>;\n\nexport async function generateInterviewQuestions(input: GenerateInterviewQuestionsInput): Promise<GenerateInterviewQuestionsOutput> {\n  return generateInterviewQuestionsFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'generateInterviewQuestionsPrompt',\n  input: {schema: GenerateInterviewQuestionsInputSchema},\n  output: {schema: GenerateInterviewQuestionsOutputSchema},\n  prompt: `You are an expert career coach specializing in helping candidates prepare for job interviews.\n\n  Based on the following information extracted from the candidate's resume, generate {{{numberOfQuestions}}} interview questions that are relevant to their experience and skills.\n  The questions should be challenging but fair, and designed to assess the candidate's suitability for a role.\n  For each question, also provide a guidanceLink. This link should be a Google search query URL formatted as 'https://www.google.com/search?q=how+to+answer+[URL_ENCODED_QUESTION_TEXT]'. Ensure the question text in the URL is properly URL encoded.\n\n  Return the interview questions as a JSON array of objects, where each object has a \"question\" field (string) and a \"guidanceLink\" field (string URL).\n\n  Resume Data: {{{resumeData}}}\n  Number of Questions: {{{numberOfQuestions}}}`,\n});\n\nconst generateInterviewQuestionsFlow = ai.defineFlow(\n  {\n    name: 'generateInterviewQuestionsFlow',\n    inputSchema: GenerateInterviewQuestionsInputSchema,\n    outputSchema: GenerateInterviewQuestionsOutputSchema,\n  },\n  async input => {\n    const {output} = await prompt(input);\n    return output!;\n  }\n);\n\nexport type { QuestionObjectSchema };\n",
        "type": "registry:component",
        "target": "ai/flows/generate-interview-questions.ts"
      },
      {
        "path": "ai/flows/parse-resume.ts",
        "content": "'use server';\n\n/**\n * @fileOverview Resume parsing AI agent.\n *\n * - parseResume - A function that handles the resume parsing process.\n * - ParseResumeInput - The input type for the parseResume function.\n * - ParseResumeOutput - The return type for the parseResume function.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst ParseResumeInputSchema = z.object({\n  resumeDataUri: z\n    .string()\n    .describe(\n      \"A resume file, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'.\"\n    ),\n});\nexport type ParseResumeInput = z.infer<typeof ParseResumeInputSchema>;\n\nconst ParseResumeOutputSchema = z.object({\n  workExperience: z\n    .array(z.string())\n    .describe('List of work experiences extracted from the resume.'),\n  skills: z.array(z.string()).describe('List of skills extracted from the resume.'),\n  projects: z\n    .array(z.string())\n    .describe('List of projects extracted from the resume.'),\n});\nexport type ParseResumeOutput = z.infer<typeof ParseResumeOutputSchema>;\n\nexport async function parseResume(input: ParseResumeInput): Promise<ParseResumeOutput> {\n  return parseResumeFlow(input);\n}\n\nconst prompt = ai.definePrompt({\n  name: 'parseResumePrompt',\n  input: {schema: ParseResumeInputSchema},\n  output: {schema: ParseResumeOutputSchema},\n  prompt: `You are a resume parsing expert. Please extract the key information from the resume, including work experience, skills, and projects. Return the data as a JSON object.\n\nResume: {{media url=resumeDataUri}}`,\n});\n\nconst parseResumeFlow = ai.defineFlow(\n  {\n    name: 'parseResumeFlow',\n    inputSchema: ParseResumeInputSchema,\n    outputSchema: ParseResumeOutputSchema,\n  },\n  async input => {\n    const {output} = await prompt(input);\n    return output!;\n  }\n);\n",
        "type": "registry:component",
        "target": "ai/flows/parse-resume.ts"
      },
      {
        "path": "ai/flows/transcribe-answer.ts",
        "content": "'use server';\n/**\n * @fileOverview This file contains the Genkit flow for transcribing recorded answers using Google Cloud Speech-to-Text.\n *\n * - transcribeAnswer - A function that transcribes audio data to text.\n * - TranscribeAnswerInput - The input type for the transcribeAnswer function, which includes the audio data URI.\n * - TranscribeAnswerOutput - The return type for the transcribeAnswer function, which includes the transcribed text.\n */\n\nimport {ai} from '@/ai/genkit';\nimport {z} from 'genkit';\n\nconst TranscribeAnswerInputSchema = z.object({\n  audioDataUri: z\n    .string()\n    .describe(\n      'The recorded answer as a data URI that must include a MIME type and use Base64 encoding. Expected format: \\'data:<mimetype>;base64,<encoded_data>\\'.' \n    ),\n});\nexport type TranscribeAnswerInput = z.infer<typeof TranscribeAnswerInputSchema>;\n\nconst TranscribeAnswerOutputSchema = z.object({\n  transcription: z.string().describe('The transcribed text of the recorded answer.'),\n});\nexport type TranscribeAnswerOutput = z.infer<typeof TranscribeAnswerOutputSchema>;\n\nexport async function transcribeAnswer(input: TranscribeAnswerInput): Promise<TranscribeAnswerOutput> {\n  return transcribeAnswerFlow(input);\n}\n\nconst transcribeAnswerPrompt = ai.definePrompt({\n  name: 'transcribeAnswerPrompt',\n  input: {schema: TranscribeAnswerInputSchema},\n  output: {schema: TranscribeAnswerOutputSchema},\n  prompt: `Transcribe the following audio recording to text:\\n\\n{{media url=audioDataUri}}`,\n});\n\nconst transcribeAnswerFlow = ai.defineFlow(\n  {\n    name: 'transcribeAnswerFlow',\n    inputSchema: TranscribeAnswerInputSchema,\n    outputSchema: TranscribeAnswerOutputSchema,\n  },\n  async input => {\n    const {output} = await transcribeAnswerPrompt(input);\n    return output!;\n  }\n);\n",
        "type": "registry:component",
        "target": "ai/flows/transcribe-answer.ts"
      },
      {
        "path": "ai/genkit.ts",
        "content": "import {genkit} from 'genkit';\nimport {googleAI} from '@genkit-ai/googleai';\n\nexport const ai = genkit({\n  plugins: [googleAI()],\n  model: 'googleai/gemini-2.0-flash',\n});\n",
        "type": "registry:component",
        "target": "ai/genkit.ts"
      },
      {
        "path": "app/globals.css",
        "content": "@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\nbody {\n  font-family: var(--font-geist-sans), Arial, Helvetica, sans-serif;\n}\n\n@layer base {\n  :root {\n    --background: 0 0% 100%; /* White #ffffff */\n    --foreground: 0 0% 20%; /* Dark Gray #333333 */\n\n    --card: 0 0% 100%;\n    --card-foreground: 0 0% 20%;\n\n    --popover: 0 0% 100%;\n    --popover-foreground: 0 0% 20%;\n\n    --primary: 207 68% 53%; /* Blue #3498db */\n    --primary-foreground: 0 0% 100%; /* White */\n\n    --secondary: 0 0% 96.1%;\n    --secondary-foreground: 0 0% 9%;\n\n    --muted: 0 0% 96.1%;\n    --muted-foreground: 0 0% 45.1%;\n\n    --accent: 28 80% 52%; /* Orange #e67e22 */\n    --accent-foreground: 0 0% 100%; /* White */\n\n    --destructive: 0 84.2% 60.2%;\n    --destructive-foreground: 0 0% 98%;\n\n    --border: 0 0% 87%; /* Lighter gray for borders */\n    --input: 0 0% 87%; /* Lighter gray for input borders */\n    --ring: 207 68% 53%; /* Primary blue for focus rings */\n\n    --chart-1: 12 76% 61%;\n    --chart-2: 173 58% 39%;\n    --chart-3: 197 37% 24%;\n    --chart-4: 43 74% 66%;\n    --chart-5: 27 87% 67%;\n    --radius: 0.5rem;\n\n    --sidebar-background: 0 0% 98%;\n    --sidebar-foreground: 240 5.3% 26.1%;\n    --sidebar-primary: 240 5.9% 10%;\n    --sidebar-primary-foreground: 0 0% 98%;\n    --sidebar-accent: 240 4.8% 95.9%;\n    --sidebar-accent-foreground: 240 5.9% 10%;\n    --sidebar-border: 220 13% 91%;\n    --sidebar-ring: 217.2 91.2% 59.8%;\n  }\n\n  .dark {\n    --background: 0 0% 3.9%;\n    --foreground: 0 0% 98%;\n\n    --card: 0 0% 3.9%;\n    --card-foreground: 0 0% 98%;\n\n    --popover: 0 0% 3.9%;\n    --popover-foreground: 0 0% 98%;\n\n    --primary: 207 68% 53%; /* Blue #3498db */\n    --primary-foreground: 0 0% 100%; /* White */\n\n    --secondary: 0 0% 14.9%;\n    --secondary-foreground: 0 0% 98%;\n\n    --muted: 0 0% 14.9%;\n    --muted-foreground: 0 0% 63.9%;\n\n    --accent: 28 80% 52%; /* Orange #e67e22 */\n    --accent-foreground: 0 0% 100%; /* White */\n\n    --destructive: 0 62.8% 30.6%;\n    --destructive-foreground: 0 0% 98%;\n\n    --border: 0 0% 20%;\n    --input: 0 0% 20%;\n    --ring: 207 68% 53%; /* Primary blue for focus rings */\n    \n    --chart-1: 220 70% 50%;\n    --chart-2: 160 60% 45%;\n    --chart-3: 30 80% 55%;\n    --chart-4: 280 65% 60%;\n    --chart-5: 340 75% 55%;\n\n    --sidebar-background: 240 5.9% 10%;\n    --sidebar-foreground: 240 4.8% 95.9%;\n    --sidebar-primary: 224.3 76.3% 48%;\n    --sidebar-primary-foreground: 0 0% 100%;\n    --sidebar-accent: 240 3.7% 15.9%;\n    --sidebar-accent-foreground: 240 4.8% 95.9%;\n    --sidebar-border: 240 3.7% 15.9%;\n    --sidebar-ring: 217.2 91.2% 59.8%;\n  }\n}\n\n@layer base {\n  * {\n    @apply border-border;\n  }\n  body {\n    @apply bg-background text-foreground;\n    min-height: 100vh;\n    display: flex;\n    flex-direction: column;\n  }\n}\n",
        "type": "registry:block",
        "target": "examples/app/globals.css"
      },
      {
        "path": "app/layout.tsx",
        "content": "import type {Metadata} from 'next';\nimport { Geist, Geist_Mono } from 'next/font/google';\nimport './globals.css';\nimport { Toaster } from \"@/components/ui/toaster\";\n\nconst geistSans = Geist({\n  variable: '--font-geist-sans',\n  subsets: ['latin'],\n});\n\nconst geistMono = Geist_Mono({\n  variable: '--font-geist-mono',\n  subsets: ['latin'],\n});\n\nexport const metadata: Metadata = {\n  title: 'Resume Interview Ace',\n  description: 'Ace your interviews with AI-powered practice.',\n};\n\nexport default function RootLayout({\n  children,\n}: Readonly<{\n  children: React.ReactNode;\n}>) {\n  return (\n    <html lang=\"en\">\n      <body className={`${geistSans.variable} ${geistMono.variable} font-sans antialiased`}>\n        {children}\n        <Toaster />\n      </body>\n    </html>\n  );\n}\n",
        "type": "registry:component",
        "target": "examples/app/layout.tsx"
      },
      {
        "path": "app/page.tsx",
        "content": "\n\"use client\";\n\nimport { useState, useEffect, useCallback } from 'react';\nimport type { ParseResumeOutput } from '@/ai/flows/parse-resume';\nimport { parseResume } from '@/ai/flows/parse-resume';\nimport type { GenerateInterviewQuestionsOutput, QuestionObjectSchema } from '@/ai/flows/generate-interview-questions';\nimport { generateInterviewQuestions } from '@/ai/flows/generate-interview-questions';\nimport type { TranscribeAnswerOutput } from '@/ai/flows/transcribe-answer';\nimport { transcribeAnswer } from '@/ai/flows/transcribe-answer';\nimport type { EvaluateAnswerOutput } from '@/ai/flows/evaluate-answer';\nimport { evaluateAnswer } from '@/ai/flows/evaluate-answer';\n// Import for facial data analysis\nimport type { AnalyzeVideoPerformanceOutput } from '@/ai/flows/analyze-video-performance';\nimport { analyzeVideoPerformance } from '@/ai/flows/analyze-video-performance';\n\n\nimport { AppHeader } from '@/components/app-header';\nimport { ResumeUploader } from '@/components/resume-uploader';\nimport { VideoRecorder } from '@/components/video-recorder';\nimport { AnswerEvaluation } from '@/components/answer-evaluation';\nimport { LoadingIndicator } from '@/components/loading-indicator';\nimport { InterviewSummary } from '@/components/interview-summary';\nimport { Button } from '@/components/ui/button';\nimport { Card, CardContent, CardDescription, CardFooter, CardHeader, CardTitle } from '@/components/ui/card';\nimport { Alert, AlertDescription, AlertTitle } from '@/components/ui/alert';\nimport { Input } from '@/components/ui/input';\nimport { Label } from '@/components/ui/label';\nimport { useToast } from '@/hooks/use-toast';\nimport { FileText, Brain, Mic, Volume2, ChevronRight, RotateCcw, CheckCircle, ListChecks, Info, AlertTriangle, ExternalLink, HelpCircle, Wand2, Video } from 'lucide-react';\n\ntype InterviewStage =\n  | 'INITIAL'\n  | 'RESUME_PARSING'\n  | 'RESUME_PARSED'\n  | 'AWAITING_NUM_QUESTIONS'\n  | 'GENERATING_QUESTIONS'\n  | 'QUESTIONS_READY'\n  | 'INTERVIEWING'\n  | 'PROCESSING_ANSWER'\n  | 'QUESTION_EVALUATED'\n  | 'INTERVIEW_COMPLETE'\n  | 'ERROR_STATE';\n\ninterface InterviewLog {\n  question: string;\n  guidanceLink?: string;\n  videoUri: string | null;\n  transcribedAnswer: string | null;\n  evaluation: EvaluateAnswerOutput | null;\n  videoAnalysis: AnalyzeVideoPerformanceOutput | null;\n}\n\nexport default function InterviewPage() {\n  const [stage, setStage] = useState<InterviewStage>('INITIAL');\n  const [parsedResumeData, setParsedResumeData] = useState<ParseResumeOutput | null>(null);\n  const [parsedResumeString, setParsedResumeString] = useState<string>('');\n  const [numberOfQuestions, setNumberOfQuestions] = useState<number>(3);\n  const [generatedQuestions, setGeneratedQuestions] = useState<QuestionObjectSchema[]>([]);\n  const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);\n  \n  const [currentRecordedVideoUri, setCurrentRecordedVideoUri] = useState<string | null>(null);\n  const [currentTranscribedAnswer, setCurrentTranscribedAnswer] = useState<string | null>(null);\n  const [currentEvaluation, setCurrentEvaluation] = useState<EvaluateAnswerOutput | null>(null);\n  const [currentVideoAnalysis, setCurrentVideoAnalysis] = useState<AnalyzeVideoPerformanceOutput | null>(null);\n  \n  const [interviewLog, setInterviewLog] = useState<InterviewLog[]>([]);\n  \n  const [isLoading, setIsLoading] = useState(false);\n  const [loadingMessage, setLoadingMessage] = useState<string>('');\n  const [errorMessage, setErrorMessage] = useState<string | null>(null);\n  \n  const { toast } = useToast();\n  const [speechSynthesisSupported, setSpeechSynthesisSupported] = useState(true);\n\n  useEffect(() => {\n    if (typeof window !== 'undefined' && !window.speechSynthesis) {\n      setSpeechSynthesisSupported(false);\n    }\n  }, []);\n\n  const handleError = useCallback((message: string, error?: any) => {\n    console.error(message, error);\n    setErrorMessage(message);\n    setStage('ERROR_STATE');\n    setIsLoading(false);\n    toast({ title: \"An Error Occurred\", description: message, variant: \"destructive\" });\n  }, [toast]);\n\n  const handleResumeUpload = useCallback(async (file: File) => {\n    setIsLoading(true);\n    setLoadingMessage('Parsing your resume...');\n    setStage('RESUME_PARSING');\n    setErrorMessage(null);\n\n    try {\n      const reader = new FileReader();\n      reader.onloadend = async () => {\n        const resumeDataUri = reader.result as string;\n        const parsedData = await parseResume({ resumeDataUri });\n        setParsedResumeData(parsedData);\n        const summaryString = `Work Experience: ${parsedData.workExperience.join(', ') || 'N/A'}. Skills: ${parsedData.skills.join(', ') || 'N/A'}. Projects: ${parsedData.projects.join(', ') || 'N/A'}.`;\n        setParsedResumeString(summaryString);\n        setStage('AWAITING_NUM_QUESTIONS');\n        setIsLoading(false);\n        toast({ title: \"Resume Parsed\", description: \"Your resume has been successfully parsed.\" });\n      };\n      reader.onerror = () => handleError('Failed to read resume file.');\n      reader.readAsDataURL(file);\n    } catch (error) {\n      handleError('Failed to parse resume.', error);\n    }\n  }, [handleError, toast]);\n\n  const handleGenerateQuestions = useCallback(async () => {\n    if (!parsedResumeString) {\n      handleError('Cannot generate questions without parsed resume data.');\n      return;\n    }\n    if (numberOfQuestions <= 0 || numberOfQuestions > 20) {\n        handleError('Please enter a valid number of questions (1-20).');\n        return;\n    }\n    setIsLoading(true);\n    setLoadingMessage('Generating interview questions...');\n    setStage('GENERATING_QUESTIONS');\n    setErrorMessage(null);\n\n    try {\n      const result: GenerateInterviewQuestionsOutput = await generateInterviewQuestions({ \n        resumeData: parsedResumeString,\n        numberOfQuestions: numberOfQuestions \n      });\n      setGeneratedQuestions(result.questions);\n      setStage('QUESTIONS_READY');\n      setIsLoading(false);\n      toast({ title: \"Questions Generated\", description: \"Your personalized interview questions are ready!\" });\n    } catch (error) {\n      handleError('Failed to generate interview questions.', error);\n    }\n  }, [parsedResumeString, handleError, toast, numberOfQuestions]);\n\n  const handleStartInterview = () => {\n    setCurrentQuestionIndex(0);\n    setInterviewLog([]);\n    setCurrentRecordedVideoUri(null);\n    setCurrentTranscribedAnswer(null);\n    setCurrentEvaluation(null);\n    setCurrentVideoAnalysis(null);\n    setStage('INTERVIEWING');\n  };\n  \n  const handleVideoSubmission = useCallback(async (videoDataUri: string, facialData: any[]) => {\n    setCurrentRecordedVideoUri(videoDataUri);\n    setIsLoading(true);\n    setLoadingMessage('Processing your answer (transcribing, evaluating & analyzing performance)...');\n    setStage('PROCESSING_ANSWER');\n    setErrorMessage(null);\n    const currentQuestionObject = generatedQuestions[currentQuestionIndex];\n\n    try {\n      if (!parsedResumeString || !currentQuestionObject?.question) {\n        throw new Error(\"Missing data for evaluation.\");\n      }\n      \n      setLoadingMessage('Analyzing your performance...');\n      \n      // Start transcription and facial data analysis in parallel\n      const transcribePromise = transcribeAnswer({ audioDataUri: videoDataUri });\n      \n      const facialDataJson = JSON.stringify(facialData);\n      const videoAnalysisPromise = analyzeVideoPerformance({ facialDataJson });\n\n      // Wait for transcription to finish, as evaluation depends on it\n      const transcriptionResult = await transcribePromise;\n      const transcription = transcriptionResult.transcription;\n      setCurrentTranscribedAnswer(transcription);\n\n      // Now start the evaluation, which needs the transcription\n      const evaluatePromise = evaluateAnswer({\n        question: currentQuestionObject.question,\n        answer: transcription,\n        resumeData: parsedResumeString,\n      });\n\n      // Wait for the remaining promises to resolve\n      const [videoAnalysisResult, finalEvaluationResult] = await Promise.all([\n        videoAnalysisPromise,\n        evaluatePromise\n      ]);\n\n      setCurrentVideoAnalysis(videoAnalysisResult);\n      setCurrentEvaluation(finalEvaluationResult);\n\n      setInterviewLog(prevLog => [\n        ...prevLog,\n        {\n          question: currentQuestionObject.question,\n          guidanceLink: currentQuestionObject.guidanceLink,\n          videoUri: videoDataUri,\n          transcribedAnswer: transcription,\n          evaluation: finalEvaluationResult,\n          videoAnalysis: videoAnalysisResult,\n        }\n      ]);\n\n      setStage('QUESTION_EVALUATED');\n      setIsLoading(false);\n      toast({ title: \"Answer Processed\", description: \"Your answer has been fully analyzed.\" });\n\n    } catch (error) {\n      handleError('Failed to process your answer.', error);\n      setInterviewLog(prevLog => [\n        ...prevLog,\n        {\n          question: currentQuestionObject?.question || \"Unknown Question\",\n          guidanceLink: currentQuestionObject?.guidanceLink,\n          videoUri: videoDataUri,\n          transcribedAnswer: currentTranscribedAnswer, \n          evaluation: null,\n          videoAnalysis: currentVideoAnalysis,\n        }\n      ]);\n    }\n  }, [generatedQuestions, currentQuestionIndex, parsedResumeString, handleError, toast, currentTranscribedAnswer, currentVideoAnalysis]);\n\n\n  const handleNextQuestion = () => {\n    if (currentQuestionIndex < generatedQuestions.length - 1) {\n      setCurrentQuestionIndex(prev => prev + 1);\n      setCurrentRecordedVideoUri(null);\n      setCurrentTranscribedAnswer(null);\n      setCurrentEvaluation(null);\n      setCurrentVideoAnalysis(null);\n      setStage('INTERVIEWING');\n    } else {\n      setStage('INTERVIEW_COMPLETE');\n    }\n  };\n\n  const handleSpeakQuestion = () => {\n    const currentQuestionText = generatedQuestions[currentQuestionIndex]?.question;\n    if (!speechSynthesisSupported || !currentQuestionText) return;\n    try {\n      const utterance = new SpeechSynthesisUtterance(currentQuestionText);\n      speechSynthesis.speak(utterance);\n    } catch (error) {\n      console.error(\"Error with speech synthesis:\", error);\n      toast({ title: \"Speech Error\", description: \"Could not read the question aloud.\", variant: \"destructive\" });\n    }\n  };\n\n  const handleRestart = () => {\n    setStage('INITIAL');\n    setParsedResumeData(null);\n    setParsedResumeString('');\n    setGeneratedQuestions([]);\n    setCurrentQuestionIndex(0);\n    setNumberOfQuestions(3);\n    setCurrentRecordedVideoUri(null);\n    setCurrentTranscribedAnswer(null);\n    setCurrentEvaluation(null);\n    setCurrentVideoAnalysis(null);\n    setInterviewLog([]);\n    setIsLoading(false);\n    setLoadingMessage('');\n    setErrorMessage(null);\n  };\n\n  const renderContent = () => {\n    if (isLoading) {\n      return <LoadingIndicator message={loadingMessage} />;\n    }\n    if (stage === 'ERROR_STATE' && errorMessage) {\n      return (\n        <Card className=\"w-full max-w-lg mx-auto text-center\">\n          <CardHeader>\n            <CardTitle className=\"text-destructive\">Error</CardTitle>\n          </CardHeader>\n          <CardContent>\n            <Alert variant=\"destructive\">\n              <AlertTriangle className=\"h-4 w-4\" />\n              <AlertTitle>An Error Occurred</AlertTitle>\n              <AlertDescription>{errorMessage}</AlertDescription>\n            </Alert>\n            <Button onClick={handleRestart} variant=\"outline\" className=\"mt-6\">\n              <RotateCcw className=\"mr-2 h-4 w-4\" /> Try Again\n            </Button>\n          </CardContent>\n        </Card>\n      );\n    }\n\n    switch (stage) {\n      case 'INITIAL':\n        return <ResumeUploader onFileUpload={handleResumeUpload} isLoading={isLoading} />;\n      \n      case 'AWAITING_NUM_QUESTIONS':\n         return (\n          <Card className=\"w-full max-w-lg mx-auto text-center shadow-lg\">\n            <CardHeader>\n              <CardTitle className=\"flex items-center justify-center gap-2\"><FileText className=\"h-6 w-6 text-primary\" />Resume Parsed!</CardTitle>\n              <CardDescription>We've extracted key information. Now, how many questions would you like?</CardDescription>\n            </CardHeader>\n            <CardContent className=\"space-y-4\">\n              {parsedResumeData && (\n                <div className=\"text-left text-sm space-y-2 bg-muted/50 p-4 rounded-md border\">\n                  <p><strong>Skills:</strong> {parsedResumeData.skills.join(', ') || 'Not found'}</p>\n                  <p><strong>Experience Snippet:</strong> {parsedResumeData.workExperience[0]?.substring(0,100) || 'Not found'}...</p>\n                </div>\n              )}\n              <div className=\"space-y-2\">\n                <Label htmlFor=\"num-questions\" className=\"text-left block\">Number of Questions (1-20):</Label>\n                <Input\n                  id=\"num-questions\"\n                  type=\"number\"\n                  value={numberOfQuestions}\n                  onChange={(e) => setNumberOfQuestions(parseInt(e.target.value, 10))}\n                  min=\"1\"\n                  max=\"20\"\n                  className=\"w-full\"\n                />\n              </div>\n            </CardContent>\n            <CardFooter>\n              <Button onClick={handleGenerateQuestions} className=\"w-full bg-accent hover:bg-accent/90 text-accent-foreground\">\n                <Wand2 className=\"mr-2 h-5 w-5\" /> Generate Interview Questions\n              </Button>\n            </CardFooter>\n          </Card>\n        );\n\n      case 'QUESTIONS_READY':\n        return (\n          <Card className=\"w-full max-w-lg mx-auto text-center shadow-lg\">\n            <CardHeader>\n              <CardTitle className=\"flex items-center justify-center gap-2\"><CheckCircle className=\"h-6 w-6 text-green-500\" />Questions Generated!</CardTitle>\n              <CardDescription>{generatedQuestions.length} personalized questions are ready for you.</CardDescription>\n            </CardHeader>\n            <CardContent>\n              <Button onClick={handleStartInterview} size=\"lg\" className=\"w-full\">\n                <Video className=\"mr-2 h-5 w-5\" /> Start Video Interview\n              </Button>\n            </CardContent>\n          </Card>\n        );\n\n      case 'INTERVIEWING':\n        const currentQuestionObject = generatedQuestions[currentQuestionIndex];\n        if (!currentQuestionObject) return <LoadingIndicator message=\"Loading question...\" />;\n        return (\n          <Card className=\"w-full max-w-2xl mx-auto shadow-xl\">\n            <CardHeader>\n              <CardTitle>Question {currentQuestionIndex + 1} of {generatedQuestions.length}</CardTitle>\n              <CardDescription className=\"text-xl py-4 text-foreground leading-relaxed\">\n                {currentQuestionObject.question}\n              </CardDescription>\n              <div className=\"flex flex-col sm:flex-row gap-2 items-start\">\n                {speechSynthesisSupported && (\n                  <Button onClick={handleSpeakQuestion} variant=\"outline\" size=\"sm\" className=\"self-start\">\n                    <Volume2 className=\"mr-2 h-4 w-4\" /> Read Aloud\n                  </Button>\n                )}\n                {currentQuestionObject.guidanceLink && (\n                  <Button asChild variant=\"outline\" size=\"sm\" className=\"self-start\">\n                    <a href={currentQuestionObject.guidanceLink} target=\"_blank\" rel=\"noopener noreferrer\">\n                      <HelpCircle className=\"mr-2 h-4 w-4\" /> Get Guidance <ExternalLink className=\"ml-1 h-3 w-3\"/>\n                    </a>\n                  </Button>\n                )}\n              </div>\n               {!speechSynthesisSupported && (\n                <Alert variant=\"default\" className=\"mt-2 text-sm\">\n                  <Info className=\"h-4 w-4\"/>\n                  <AlertDescription>\n                  Text-to-speech is not available in your browser. Please read the question manually.\n                  </AlertDescription>\n                </Alert>\n              )}\n            </CardHeader>\n            <CardContent>\n              <VideoRecorder onRecordingComplete={handleVideoSubmission} isProcessing={isLoading} />\n            </CardContent>\n          </Card>\n        );\n\n      case 'QUESTION_EVALUATED':\n        return (\n          <div className=\"w-full max-w-2xl mx-auto space-y-6\">\n            <AnswerEvaluation \n              questionText={generatedQuestions[currentQuestionIndex]?.question}\n              transcribedText={currentTranscribedAnswer}\n              evaluation={currentEvaluation?.evaluation ?? null}\n              score={currentEvaluation?.score ?? null}\n              followUpQuestion={currentEvaluation?.followUpQuestion ?? null}\n              expectedAnswerElements={currentEvaluation?.expectedAnswerElements ?? null}\n              suggestedResources={currentEvaluation?.suggestedResources ?? null}\n              videoAnalysis={currentVideoAnalysis}\n            />\n            <Button onClick={handleNextQuestion} size=\"lg\" className=\"w-full\">\n              {currentQuestionIndex < generatedQuestions.length - 1 ? 'Next Question' : 'Finish Interview'}\n              <ChevronRight className=\"ml-2 h-5 w-5\" />\n            </Button>\n          </div>\n        );\n      \n      case 'INTERVIEW_COMPLETE':\n        return (\n          <div className=\"w-full max-w-3xl mx-auto space-y-6\">\n            <InterviewSummary interviewData={interviewLog} />\n            <Button onClick={handleRestart} variant=\"outline\" size=\"lg\" className=\"w-full\">\n              <RotateCcw className=\"mr-2 h-5 w-5\" /> Start New Interview\n            </Button>\n          </div>\n        );\n        \n      default: // Includes RESUME_PARSING, GENERATING_QUESTIONS, PROCESSING_ANSWER\n        return <LoadingIndicator message={loadingMessage || \"Loading...\"} />;\n    }\n  };\n\n  return (\n    <div className=\"flex flex-col min-h-screen bg-background\">\n      <AppHeader />\n      <main className=\"flex-grow container mx-auto px-4 py-8 md:py-12 flex items-center justify-center\">\n        {renderContent()}\n      </main>\n      <footer className=\"text-center py-4 border-t text-sm text-muted-foreground\">\n        <p>&copy; {new Date().getFullYear()} Resume Interview Ace. Level up your interview skills.</p>\n      </footer>\n    </div>\n  );\n}\n",
        "type": "registry:page",
        "target": "examples/app/page.tsx"
      },
      {
        "path": "components/answer-evaluation.tsx",
        "content": "\nimport type { SuggestedResourceSchema } from \"@/ai/flows/evaluate-answer\";\nimport type { AnalyzeVideoPerformanceOutput } from \"@/ai/flows/analyze-video-performance\";\nimport { Card, CardContent, CardDescription, CardHeader, CardTitle } from \"@/components/ui/card\";\nimport { Separator } from \"@/components/ui/separator\";\nimport { Badge } from \"@/components/ui/badge\";\nimport { Lightbulb, MessageSquareText, CheckCircle2, Star, HelpCircle, BookOpen, ExternalLink, Target, Smile, AlertCircle, Eye } from \"lucide-react\";\nimport ReactMarkdown from 'react-markdown';\nimport { Alert, AlertDescription, AlertTitle } from \"./ui/alert\";\n\ninterface AnswerEvaluationProps {\n  questionText?: string | null;\n  transcribedText: string | null;\n  evaluation: string | null;\n  score: number | null;\n  followUpQuestion: string | null;\n  expectedAnswerElements?: string | null;\n  suggestedResources?: SuggestedResourceSchema[] | null;\n  videoAnalysis?: AnalyzeVideoPerformanceOutput | null;\n}\n\nexport function AnswerEvaluation({ \n  questionText, \n  transcribedText, \n  evaluation, \n  score, \n  followUpQuestion,\n  expectedAnswerElements,\n  suggestedResources,\n  videoAnalysis\n}: AnswerEvaluationProps) {\n\n  if (!transcribedText && !evaluation && !followUpQuestion && score === null && !expectedAnswerElements && !suggestedResources && !videoAnalysis) {\n    return null; \n  }\n\n  const getScoreColor = (currentScore: number | null) => {\n    if (currentScore === null) return \"bg-gray-500\";\n    if (currentScore >= 8) return \"bg-green-500\";\n    if (currentScore >= 5) return \"bg-yellow-500\";\n    return \"bg-red-500\";\n  };\n\n  return (\n    <Card className=\"w-full shadow-lg\">\n      <CardHeader>\n        <div className=\"flex justify-between items-start\">\n          <div>\n            <CardTitle className=\"flex items-center gap-2\">\n              <CheckCircle2 className=\"h-6 w-6 text-primary\" />\n              Answer Feedback\n            </CardTitle>\n            <CardDescription>Here's the analysis of your response.</CardDescription>\n          </div>\n          {score !== null && (\n             <Badge variant=\"secondary\" className={`px-3 py-1 text-lg font-semibold text-white ${getScoreColor(score)}`}>\n                Content Score: {score}/10\n             </Badge>\n          )}\n        </div>\n      </CardHeader>\n      <CardContent className=\"space-y-6\">\n        {videoAnalysis && (\n            <div>\n              <h3 className=\"text-lg font-semibold mb-2 flex items-center gap-2\">\n                <Smile className=\"h-5 w-5 text-indigo-500\" />\n                Visual Performance Analysis\n              </h3>\n              <div className=\"space-y-4 rounded-md border bg-muted/30 p-4\">\n                  <div className=\"flex justify-between items-center\">\n                      <p className=\"font-medium\">Confidence Score</p>\n                      <Badge variant=\"secondary\" className={`text-base ${getScoreColor(videoAnalysis.confidenceScore)}`}>\n                          {videoAnalysis.confidenceScore}/10\n                      </Badge>\n                  </div>\n                   <Separator />\n                   <div>\n                       <p className=\"font-medium mb-1\">Nervousness</p>\n                       <p className=\"text-sm text-muted-foreground\">{videoAnalysis.nervousnessAnalysis}</p>\n                   </div>\n                   <div>\n                       <p className=\"font-medium mb-1 flex items-center gap-2\"><Eye className=\"h-4 w-4\"/>Gaze & Focus</p>\n                       <p className=\"text-sm text-muted-foreground\">{videoAnalysis.gazeAnalysis}</p>\n                   </div>\n                   {videoAnalysis.cheatingSuspicion && (\n                       <Alert variant=\"destructive\">\n                            <AlertCircle className=\"h-4 w-4\" />\n                            <AlertTitle>Potential Cheating Flagged</AlertTitle>\n                            <AlertDescription>\n                                The system detected consistent gaze away from the screen, which could indicate reading from notes. Be sure to maintain eye contact with the camera.\n                            </AlertDescription>\n                       </Alert>\n                   )}\n              </div>\n            </div>\n        )}\n\n        {questionText && (\n           <div>\n            {videoAnalysis && <Separator className=\"my-4\" /> }\n            <h3 className=\"text-lg font-semibold mb-2 flex items-center gap-2\">\n              <HelpCircle className=\"h-5 w-5 text-muted-foreground\" />\n              Question Asked\n            </h3>\n            <p className=\"text-muted-foreground leading-relaxed bg-muted/30 p-3 rounded-md\">\n              {questionText}\n            </p>\n          </div>\n        )}\n        {transcribedText && (\n          <div>\n            {(questionText || videoAnalysis) && <Separator className=\"my-4\" /> }\n            <h3 className=\"text-lg font-semibold mb-2 flex items-center gap-2\">\n              <MessageSquareText className=\"h-5 w-5 text-primary\" />\n              Your Answer (Transcribed)\n            </h3>\n            <blockquote className=\"pl-4 border-l-4 border-primary/50 italic text-foreground bg-primary/10 p-3 rounded-md\">\n              {transcribedText}\n            </blockquote>\n          </div>\n        )}\n\n        {evaluation && (\n          <div>\n            <Separator className=\"my-4\" />\n            <h3 className=\"text-lg font-semibold mb-2 flex items-center gap-2\">\n              <Star className=\"h-5 w-5 text-yellow-500\" />\n              AI Evaluation (Content)\n            </h3>\n            <p className=\"text-foreground leading-relaxed whitespace-pre-wrap bg-green-50 border border-green-200 p-3 rounded-md\">\n              {evaluation}\n            </p>\n          </div>\n        )}\n\n        {expectedAnswerElements && (\n          <div>\n            <Separator className=\"my-4\" />\n            <h3 className=\"text-lg font-semibold mb-2 flex items-center gap-2\">\n              <Target className=\"h-5 w-5 text-blue-500\" />\n              Expected Key Points\n            </h3>\n            <div className=\"text-foreground leading-relaxed bg-blue-50 border border-blue-200 p-3 rounded-md prose prose-sm max-w-none\">\n              <ReactMarkdown>{expectedAnswerElements}</ReactMarkdown>\n            </div>\n          </div>\n        )}\n        \n        {suggestedResources && suggestedResources.length > 0 && (\n          <div>\n            <Separator className=\"my-4\" />\n            <h3 className=\"text-lg font-semibold mb-2 flex items-center gap-2\">\n              <BookOpen className=\"h-5 w-5 text-purple-500\" />\n              Suggested Learning Resources\n            </h3>\n            <ul className=\"space-y-2\">\n              {suggestedResources.map((resource, index) => (\n                <li key={index} className=\"text-sm bg-purple-50 border border-purple-200 p-3 rounded-md\">\n                  <a \n                    href={resource.url} \n                    target=\"_blank\" \n                    rel=\"noopener noreferrer\" \n                    className=\"font-medium text-purple-700 hover:text-purple-900 hover:underline flex items-center gap-1\"\n                  >\n                    {resource.title}\n                    <ExternalLink className=\"h-3 w-3\" />\n                  </a>\n                </li>\n              ))}\n            </ul>\n          </div>\n        )}\n\n        {followUpQuestion && (\n          <div>\n            <Separator className=\"my-4\" />\n            <h3 className=\"text-lg font-semibold mb-2 flex items-center gap-2\">\n              <Lightbulb className=\"h-5 w-5 text-accent\" />\n              Suggested Follow-up Question\n            </h3>\n            <p className=\"text-foreground leading-relaxed whitespace-pre-wrap bg-orange-50 border border-orange-200 p-3 rounded-md\">\n              {followUpQuestion}\n            </p>\n          </div>\n        )}\n      </CardContent>\n    </Card>\n  );\n}\n",
        "type": "registry:component",
        "target": "components/answer-evaluation.tsx"
      },
      {
        "path": "components/app-header.tsx",
        "content": "import { Briefcase } from 'lucide-react';\n\nexport function AppHeader() {\n  return (\n    <header className=\"py-6 px-4 md:px-8 border-b\">\n      <div className=\"container mx-auto flex items-center gap-3\">\n        <Briefcase className=\"h-8 w-8 text-primary\" />\n        <h1 className=\"text-3xl font-bold text-foreground\">\n          Resume Interview Ace\n        </h1>\n      </div>\n    </header>\n  );\n}\n",
        "type": "registry:component",
        "target": "components/app-header.tsx"
      },
      {
        "path": "components/interview-summary.tsx",
        "content": "\nimport type { EvaluateAnswerOutput, SuggestedResourceSchema } from \"@/ai/flows/evaluate-answer\";\nimport type { AnalyzeVideoPerformanceOutput } from \"@/ai/flows/analyze-video-performance\";\nimport { Card, CardContent, CardDescription, CardHeader, CardTitle } from \"@/components/ui/card\";\nimport { Accordion, AccordionContent, AccordionItem, AccordionTrigger } from \"@/components/ui/accordion\";\nimport { Badge } from \"@/components/ui/badge\";\nimport { CheckCircle, MessageCircle, HelpCircle, Mic, Star, Percent, TrendingUp, ExternalLink, Target, BookOpen, Lightbulb, Smile, Eye, AlertCircle } from \"lucide-react\";\nimport ReactMarkdown from 'react-markdown';\n\ninterface InterviewData {\n  question: string;\n  guidanceLink?: string;\n  transcribedAnswer: string | null;\n  evaluation: EvaluateAnswerOutput | null;\n  videoUri?: string | null;\n  videoAnalysis: AnalyzeVideoPerformanceOutput | null;\n}\n\ninterface InterviewSummaryProps {\n  interviewData: InterviewData[];\n}\n\nexport function InterviewSummary({ interviewData }: InterviewSummaryProps) {\n  const totalQuestions = interviewData.length;\n  const answeredQuestions = interviewData.filter(item => item.evaluation?.score !== undefined && item.evaluation.score !== null);\n  const totalScore = answeredQuestions.reduce((sum, item) => sum + (item.evaluation?.score || 0), 0);\n  const averageScore = totalQuestions > 0 && answeredQuestions.length > 0 ? (totalScore / answeredQuestions.length) : null;\n  \n  const totalConfidenceScore = interviewData.reduce((sum, item) => sum + (item.videoAnalysis?.confidenceScore || 0), 0);\n  const answeredConfidenceQuestions = interviewData.filter(item => item.videoAnalysis?.confidenceScore !== undefined && item.videoAnalysis.confidenceScore !== null);\n  const averageConfidenceScore = totalQuestions > 0 && answeredConfidenceQuestions.length > 0 ? (totalConfidenceScore / answeredConfidenceQuestions.length) : null;\n\n\n  const getScoreColor = (score: number | null | undefined) => {\n    if (score === null || score === undefined) return \"bg-gray-400\";\n    if (score >= 8) return \"bg-green-500\";\n    if (score >= 5) return \"bg-yellow-500\";\n    return \"bg-red-500\";\n  };\n  \n  return (\n    <Card className=\"w-full shadow-xl\">\n      <CardHeader>\n        <div className=\"flex flex-col sm:flex-row justify-between items-start gap-4\">\n          <div>\n            <CardTitle className=\"flex items-center gap-2\">\n              <TrendingUp className=\"h-7 w-7 text-primary\" />\n              Interview Performance Summary\n            </CardTitle>\n            <CardDescription>Review your overall performance and details for each question.</CardDescription>\n          </div>\n          <div className=\"flex gap-4\">\n          {averageScore !== null && (\n            <div className=\"text-center p-3 rounded-lg bg-primary/10\">\n              <p className=\"text-sm font-medium text-primary\">Avg. Content Score</p>\n              <p className={`text-3xl font-bold ${getScoreColor(averageScore).replace('bg-','text-')}`}>\n                {averageScore.toFixed(1)}<span className=\"text-lg\">/10</span>\n              </p>\n            </div>\n          )}\n          {averageConfidenceScore !== null && (\n            <div className=\"text-center p-3 rounded-lg bg-indigo-500/10\">\n              <p className=\"text-sm font-medium text-indigo-500\">Avg. Confidence</p>\n              <p className={`text-3xl font-bold ${getScoreColor(averageConfidenceScore).replace('bg-','text-')}`}>\n                {averageConfidenceScore.toFixed(1)}<span className=\"text-lg\">/10</span>\n              </p>\n            </div>\n          )}\n          </div>\n        </div>\n      </CardHeader>\n      <CardContent>\n        {interviewData.length === 0 ? (\n          <p className=\"text-muted-foreground\">No interview data to display.</p>\n        ) : (\n          <Accordion type=\"single\" collapsible className=\"w-full\" defaultValue={`item-0`}>\n            {interviewData.map((item, index) => (\n              <AccordionItem value={`item-${index}`} key={index}>\n                <AccordionTrigger className=\"text-lg hover:no-underline\">\n                  <div className=\"flex items-center justify-between w-full gap-3\">\n                    <div className=\"flex items-center gap-3 text-left\">\n                        <HelpCircle className=\"h-5 w-5 text-primary shrink-0\" />\n                        <span>Question {index + 1}</span>\n                    </div>\n                    <div className=\"flex gap-2 flex-wrap justify-end\">\n                      {item.videoAnalysis?.confidenceScore !== null && item.videoAnalysis?.confidenceScore !== undefined && (\n                        <Badge variant=\"outline\" className={`text-sm font-semibold border-2 ${getScoreColor(item.videoAnalysis.confidenceScore).replace('bg-','border-')}`}>\n                          Confidence: {item.videoAnalysis.confidenceScore}/10\n                        </Badge>\n                      )}\n                      {item.evaluation?.score !== null && item.evaluation?.score !== undefined && (\n                        <Badge variant=\"outline\" className={`text-sm font-semibold border-2 ${getScoreColor(item.evaluation.score).replace('bg-','border-')}`}>\n                          Content: {item.evaluation.score}/10\n                        </Badge>\n                      )}\n                    </div>\n                  </div>\n                </AccordionTrigger>\n                <AccordionContent className=\"space-y-4 pt-2 pb-4 px-2\">\n                  <p className=\"font-semibold text-base\">{item.question}</p>\n                  \n                  {item.videoUri && (\n                     <div className=\"mt-2\">\n                        <h4 className=\"font-semibold mb-1 text-xs text-muted-foreground\">Recorded Video:</h4>\n                        <video controls src={item.videoUri} className=\"w-full max-w-sm rounded-md\" />\n                    </div>\n                  )}\n\n                  {item.videoAnalysis && (\n                    <div>\n                      <h4 className=\"font-semibold mb-1 flex items-center gap-2 text-muted-foreground text-sm\">\n                        <Smile className=\"h-4 w-4 text-indigo-500\" /> Visual Analysis:\n                      </h4>\n                      <div className=\"text-sm space-y-2 bg-indigo-50 border border-indigo-100 p-2 rounded-sm\">\n                        <p><strong>Nervousness:</strong> {item.videoAnalysis.nervousnessAnalysis}</p>\n                        <p><strong>Gaze & Focus:</strong> {item.videoAnalysis.gazeAnalysis}</p>\n                        {item.videoAnalysis.cheatingSuspicion && (\n                            <p className=\"font-semibold text-destructive-foreground bg-destructive p-2 rounded-md flex items-center gap-2\"><AlertCircle className=\"h-4 w-4\"/> Cheating Flagged</p>\n                        )}\n                      </div>\n                    </div>\n                  )}\n\n                  {item.transcribedAnswer && (\n                    <div>\n                      <h4 className=\"font-semibold mb-1 flex items-center gap-2 text-muted-foreground text-sm\">\n                        <Mic className=\"h-4 w-4\" /> Your Answer:\n                      </h4>\n                      <blockquote className=\"pl-3 border-l-2 border-primary/30 italic text-foreground bg-primary/5 p-2 rounded-sm text-sm\">\n                        {item.transcribedAnswer}\n                      </blockquote>\n                    </div>\n                  )}\n                  \n                  {item.evaluation && (\n                    <>\n                      <div>\n                        <h4 className=\"font-semibold mb-1 flex items-center gap-2 text-muted-foreground text-sm\">\n                          <Star className=\"h-4 w-4 text-yellow-500\" /> Content Evaluation:\n                        </h4>\n                        <p className=\"text-sm whitespace-pre-wrap bg-green-50 border border-green-100 p-2 rounded-sm\">{item.evaluation.evaluation}</p>\n                      </div>\n                      \n                      {item.evaluation.expectedAnswerElements && (\n                        <div className=\"mt-2\">\n                          <h4 className=\"font-semibold mb-1 flex items-center gap-2 text-muted-foreground text-sm\">\n                            <Target className=\"h-4 w-4 text-blue-500\" /> Expected Key Points:\n                          </h4>\n                          <div className=\"text-sm bg-blue-50 border border-blue-100 p-2 rounded-sm prose prose-sm max-w-none\">\n                            <ReactMarkdown>{item.evaluation.expectedAnswerElements}</ReactMarkdown>\n                          </div>\n                        </div>\n                      )}\n                      {item.evaluation.suggestedResources && item.evaluation.suggestedResources.length > 0 && (\n                        <div className=\"mt-2\">\n                          <h4 className=\"font-semibold mb-1 flex items-center gap-2 text-muted-foreground text-sm\">\n                            <BookOpen className=\"h-4 w-4 text-purple-500\" /> Suggested Resources:\n                          </h4>\n                          <ul className=\"space-y-1 pl-2\">\n                            {item.evaluation.suggestedResources.map((resource, rIndex) => (\n                              <li key={rIndex} className=\"text-sm\">\n                                <a \n                                  href={resource.url} \n                                  target=\"_blank\" \n                                  rel=\"noopener noreferrer\" \n                                  className=\"text-purple-600 hover:text-purple-800 hover:underline flex items-center gap-1\"\n                                >\n                                  {resource.title}\n                                  <ExternalLink className=\"h-3 w-3\" />\n                                </a>\n                              </li>\n                            ))}\n                          </ul>\n                        </div>\n                      )}\n                      {item.evaluation.followUpQuestion && (\n                        <div className=\"mt-2\">\n                          <h4 className=\"font-semibold mb-1 flex items-center gap-2 text-muted-foreground text-sm\">\n                            <Lightbulb className=\"h-4 w-4 text-accent\" /> Suggested Follow-up:\n                          </h4>\n                          <p className=\"text-sm whitespace-pre-wrap bg-orange-50 border border-orange-100 p-2 rounded-sm\">{item.evaluation.followUpQuestion}</p>\n                        </div>\n                      )}\n                    </>\n                  )}\n                  {!item.transcribedAnswer && !item.evaluation && (\n                    <p className=\"text-sm text-muted-foreground\">No answer recorded or evaluated for this question.</p>\n                  )}\n                </AccordionContent>\n              </AccordionItem>\n            ))}\n          </Accordion>\n        )}\n      </CardContent>\n    </Card>\n  );\n}\n",
        "type": "registry:component",
        "target": "components/interview-summary.tsx"
      },
      {
        "path": "components/loading-indicator.tsx",
        "content": "import { Loader2 } from 'lucide-react';\n\ninterface LoadingIndicatorProps {\n  message?: string;\n  size?: number;\n}\n\nexport function LoadingIndicator({ message, size = 48 }: LoadingIndicatorProps) {\n  return (\n    <div className=\"flex flex-col items-center justify-center gap-4 p-8 text-muted-foreground\">\n      <Loader2 className=\"animate-spin text-primary\" style={{ width: size, height: size }} />\n      {message && <p className=\"text-lg\">{message}</p>}\n    </div>\n  );\n}\n",
        "type": "registry:component",
        "target": "components/loading-indicator.tsx"
      },
      {
        "path": "components/resume-uploader.tsx",
        "content": "\"use client\";\n\nimport type { ChangeEvent } from 'react';\nimport { useState } from 'react';\nimport { Button } from '@/components/ui/button';\nimport { Input } from '@/components/ui/input';\nimport { Label } from '@/components/ui/label';\nimport { UploadCloud, FileText } from 'lucide-react';\nimport { useToast } from '@/hooks/use-toast';\n\ninterface ResumeUploaderProps {\n  onFileUpload: (file: File) => void;\n  isLoading: boolean;\n}\n\nexport function ResumeUploader({ onFileUpload, isLoading }: ResumeUploaderProps) {\n  const [selectedFile, setSelectedFile] = useState<File | null>(null);\n  const { toast } = useToast();\n\n  const handleFileChange = (event: ChangeEvent<HTMLInputElement>) => {\n    const file = event.target.files?.[0];\n    if (file) {\n      if (file.type === 'application/pdf' || file.type === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') {\n        setSelectedFile(file);\n      } else {\n        toast({\n          title: \"Invalid File Type\",\n          description: \"Please upload a PDF or DOCX file.\",\n          variant: \"destructive\",\n        });\n        setSelectedFile(null);\n        event.target.value = \"\"; // Reset file input\n      }\n    }\n  };\n\n  const handleSubmit = () => {\n    if (selectedFile) {\n      onFileUpload(selectedFile);\n    } else {\n      toast({\n        title: \"No File Selected\",\n        description: \"Please select your resume to upload.\",\n        variant: \"destructive\",\n      });\n    }\n  };\n\n  return (\n    <div className=\"w-full max-w-md mx-auto space-y-6 p-2\">\n      <div className=\"space-y-2 text-center\">\n        <UploadCloud className=\"mx-auto h-12 w-12 text-primary\" />\n        <h2 className=\"text-2xl font-semibold\">Upload Your Resume</h2>\n        <p className=\"text-muted-foreground\">\n          Upload your resume (PDF or DOCX) to get started.\n        </p>\n      </div>\n      <div className=\"space-y-3\">\n        <Label htmlFor=\"resume-file\" className=\"sr-only\">Resume File</Label>\n        <Input\n          id=\"resume-file\"\n          type=\"file\"\n          accept=\".pdf,.docx\"\n          onChange={handleFileChange}\n          disabled={isLoading}\n          className=\"file:mr-4 file:py-2 file:px-4 file:rounded-full file:border-0 file:text-sm file:font-semibold file:bg-primary/10 file:text-primary hover:file:bg-primary/20\"\n        />\n        {selectedFile && (\n          <div className=\"flex items-center space-x-2 text-sm text-muted-foreground p-2 border rounded-md\">\n            <FileText className=\"h-5 w-5 text-primary\" />\n            <span>{selectedFile.name} ({(selectedFile.size / 1024).toFixed(1)} KB)</span>\n          </div>\n        )}\n      </div>\n      <Button\n        onClick={handleSubmit}\n        disabled={isLoading || !selectedFile}\n        className=\"w-full bg-accent hover:bg-accent/90 text-accent-foreground\"\n      >\n        {isLoading ? 'Processing...' : 'Upload and Analyze Resume'}\n      </Button>\n    </div>\n  );\n}\n",
        "type": "registry:component",
        "target": "components/resume-uploader.tsx"
      },
      {
        "path": "components/ui/toast.tsx",
        "content": "\"use client\"\n\nimport * as React from \"react\"\nimport * as ToastPrimitives from \"@radix-ui/react-toast\"\nimport { cva, type VariantProps } from \"class-variance-authority\"\nimport { X } from \"lucide-react\"\n\nimport { cn } from \"@/lib/utils\"\n\nconst ToastProvider = ToastPrimitives.Provider\n\nconst ToastViewport = React.forwardRef<\n  React.ElementRef<typeof ToastPrimitives.Viewport>,\n  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>\n>(({ className, ...props }, ref) => (\n  <ToastPrimitives.Viewport\n    ref={ref}\n    className={cn(\n      \"fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]\",\n      className\n    )}\n    {...props}\n  />\n))\nToastViewport.displayName = ToastPrimitives.Viewport.displayName\n\nconst toastVariants = cva(\n  \"group pointer-events-auto relative flex w-full items-center justify-between space-x-4 overflow-hidden rounded-md border p-6 pr-8 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full\",\n  {\n    variants: {\n      variant: {\n        default: \"border bg-background text-foreground\",\n        destructive:\n          \"destructive group border-destructive bg-destructive text-destructive-foreground\",\n      },\n    },\n    defaultVariants: {\n      variant: \"default\",\n    },\n  }\n)\n\nconst Toast = React.forwardRef<\n  React.ElementRef<typeof ToastPrimitives.Root>,\n  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> &\n    VariantProps<typeof toastVariants>\n>(({ className, variant, ...props }, ref) => {\n  return (\n    <ToastPrimitives.Root\n      ref={ref}\n      className={cn(toastVariants({ variant }), className)}\n      {...props}\n    />\n  )\n})\nToast.displayName = ToastPrimitives.Root.displayName\n\nconst ToastAction = React.forwardRef<\n  React.ElementRef<typeof ToastPrimitives.Action>,\n  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>\n>(({ className, ...props }, ref) => (\n  <ToastPrimitives.Action\n    ref={ref}\n    className={cn(\n      \"inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium ring-offset-background transition-colors hover:bg-secondary focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive\",\n      className\n    )}\n    {...props}\n  />\n))\nToastAction.displayName = ToastPrimitives.Action.displayName\n\nconst ToastClose = React.forwardRef<\n  React.ElementRef<typeof ToastPrimitives.Close>,\n  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>\n>(({ className, ...props }, ref) => (\n  <ToastPrimitives.Close\n    ref={ref}\n    className={cn(\n      \"absolute right-2 top-2 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-2 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600\",\n      className\n    )}\n    toast-close=\"\"\n    {...props}\n  >\n    <X className=\"h-4 w-4\" />\n  </ToastPrimitives.Close>\n))\nToastClose.displayName = ToastPrimitives.Close.displayName\n\nconst ToastTitle = React.forwardRef<\n  React.ElementRef<typeof ToastPrimitives.Title>,\n  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>\n>(({ className, ...props }, ref) => (\n  <ToastPrimitives.Title\n    ref={ref}\n    className={cn(\"text-sm font-semibold\", className)}\n    {...props}\n  />\n))\nToastTitle.displayName = ToastPrimitives.Title.displayName\n\nconst ToastDescription = React.forwardRef<\n  React.ElementRef<typeof ToastPrimitives.Description>,\n  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>\n>(({ className, ...props }, ref) => (\n  <ToastPrimitives.Description\n    ref={ref}\n    className={cn(\"text-sm opacity-90\", className)}\n    {...props}\n  />\n))\nToastDescription.displayName = ToastPrimitives.Description.displayName\n\ntype ToastProps = React.ComponentPropsWithoutRef<typeof Toast>\n\ntype ToastActionElement = React.ReactElement<typeof ToastAction>\n\nexport {\n  type ToastProps,\n  type ToastActionElement,\n  ToastProvider,\n  ToastViewport,\n  Toast,\n  ToastTitle,\n  ToastDescription,\n  ToastClose,\n  ToastAction,\n}\n",
        "type": "registry:component",
        "target": "components/ui/toast.tsx"
      },
      {
        "path": "components/ui/toaster.tsx",
        "content": "\"use client\"\n\nimport { useToast } from \"@/hooks/use-toast\"\nimport {\n  Toast,\n  ToastClose,\n  ToastDescription,\n  ToastProvider,\n  ToastTitle,\n  ToastViewport,\n} from \"@/components/ui/toast\"\n\nexport function Toaster() {\n  const { toasts } = useToast()\n\n  return (\n    <ToastProvider>\n      {toasts.map(function ({ id, title, description, action, ...props }) {\n        return (\n          <Toast key={id} {...props}>\n            <div className=\"grid gap-1\">\n              {title && <ToastTitle>{title}</ToastTitle>}\n              {description && (\n                <ToastDescription>{description}</ToastDescription>\n              )}\n            </div>\n            {action}\n            <ToastClose />\n          </Toast>\n        )\n      })}\n      <ToastViewport />\n    </ToastProvider>\n  )\n}\n",
        "type": "registry:component",
        "target": "components/ui/toaster.tsx"
      },
      {
        "path": "components/video-recorder.tsx",
        "content": "\n\"use client\";\n\nimport type { FC } from 'react';\nimport { useState, useRef, useEffect, useCallback } from 'react';\nimport { Button } from '@/components/ui/button';\nimport { Mic, StopCircle, Video, VideoOff, Loader2 } from 'lucide-react';\nimport { useToast } from '@/hooks/use-toast';\nimport { FaceLandmarker, FilesetResolver, FaceLandmarkerResult } from '@mediapipe/tasks-vision';\n\ninterface VideoRecorderProps {\n  onRecordingComplete: (videoDataUri: string, facialData: any[]) => void;\n  isProcessing: boolean; // True if parent is processing video\n}\n\nexport const VideoRecorder: FC<VideoRecorderProps> = ({ onRecordingComplete, isProcessing }) => {\n  const [isRecording, setIsRecording] = useState(false);\n  const [videoDataUri, setVideoDataUri] = useState<string | null>(null);\n  const [hasCameraPermission, setHasCameraPermission] = useState<boolean | null>(null);\n  const [isBrowserSupported, setIsBrowserSupported] = useState(true);\n  const [faceLandmarker, setFaceLandmarker] = useState<FaceLandmarker | null>(null);\n  const [isInitializing, setIsInitializing] = useState(true);\n\n  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n  const videoChunksRef = useRef<Blob[]>([]);\n  const videoRef = useRef<HTMLVideoElement>(null);\n  const streamRef = useRef<MediaStream | null>(null);\n  const animationFrameIdRef = useRef<number | null>(null);\n  const facialDataRef = useRef<any[]>([]);\n  const isRecordingRef = useRef(isRecording);\n  \n  const { toast } = useToast();\n\n  useEffect(() => {\n    isRecordingRef.current = isRecording;\n  }, [isRecording]);\n\n  // 1. Initialize MediaPipe FaceLandmarker\n  useEffect(() => {\n    const createFaceLandmarker = async () => {\n      try {\n        const filesetResolver = await FilesetResolver.forVisionTasks(\n          \"https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm\"\n        );\n        const landmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n          baseOptions: {\n            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,\n            delegate: \"GPU\"\n          },\n          outputFaceBlendshapes: true,\n          runningMode: \"VIDEO\",\n          numFaces: 1\n        });\n        setFaceLandmarker(landmarker);\n      } catch (error) {\n        console.error(\"Error creating FaceLandmarker:\", error);\n        toast({\n          title: \"AI Model Error\",\n          description: \"Could not load facial recognition models.\",\n          variant: \"destructive\"\n        });\n      } finally {\n        setIsInitializing(false);\n      }\n    };\n    \n    if (typeof window !== 'undefined' && (!navigator.mediaDevices || !window.MediaRecorder)) {\n        setIsBrowserSupported(false);\n        setIsInitializing(false);\n        return;\n    }\n\n    createFaceLandmarker();\n  }, [toast]);\n  \n  // 2. Main prediction loop\n  const predictWebcam = useCallback(() => {\n    const video = videoRef.current;\n    if (!video || !faceLandmarker) {\n      if (animationFrameIdRef.current) {\n        window.requestAnimationFrame(predictWebcam);\n      }\n      return;\n    }\n\n    // If the video is playing, start detecting\n    if (video.readyState >= 2) { // Check if video has enough data to play\n        try {\n            const startTimeMs = performance.now();\n            const results: FaceLandmarkerResult = faceLandmarker.detectForVideo(video, startTimeMs);\n    \n            // Add a null check for robustness\n            if(results) {\n              // If recording, save the results\n              if (isRecordingRef.current && results.faceBlendshapes && results.faceBlendshapes.length > 0) {\n                facialDataRef.current.push({\n                  timestamp: Date.now(),\n                  blendshapes: results.faceBlendshapes[0].categories,\n                });\n              } else if (isRecordingRef.current) {\n                facialDataRef.current.push(null);\n              }\n            }\n        } catch (error) {\n            console.error(\"Error during facial detection:\", error);\n        }\n    }\n\n    // Call this function again to keep predicting when the browser is ready.\n    animationFrameIdRef.current = window.requestAnimationFrame(predictWebcam);\n  }, [faceLandmarker]);\n\n  // 3. Get camera permissions and start the prediction loop\n  const enableCam = useCallback(async () => {\n    if (!faceLandmarker || hasCameraPermission) return;\n\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });\n      streamRef.current = stream;\n      if (videoRef.current) {\n        videoRef.current.srcObject = stream;\n        videoRef.current.addEventListener(\"loadeddata\", () => {\n            animationFrameIdRef.current = window.requestAnimationFrame(predictWebcam);\n        });\n      }\n      setHasCameraPermission(true);\n    } catch (error) {\n      console.error('Error accessing camera:', error);\n      setHasCameraPermission(false);\n      toast({\n        variant: 'destructive',\n        title: 'Camera Access Denied',\n        description: 'Please enable camera permissions in your browser settings.',\n      });\n    }\n  }, [faceLandmarker, hasCameraPermission, predictWebcam, toast]);\n\n  // Effect to automatically enable camera once ready\n  useEffect(() => {\n    if (!isInitializing) {\n        enableCam();\n    }\n  }, [isInitializing, enableCam]);\n\n  // Effect to handle component cleanup ONCE on unmount\n  useEffect(() => {\n    return () => {\n        if (streamRef.current) {\n            streamRef.current.getTracks().forEach(track => track.stop());\n        }\n        if (animationFrameIdRef.current) {\n            window.cancelAnimationFrame(animationFrameIdRef.current);\n            animationFrameIdRef.current = null;\n        }\n    }\n  }, []); // Empty dependency array ensures this runs only on unmount\n\n\n  const startRecording = () => {\n    if (!streamRef.current || !streamRef.current.active) {\n        toast({ title: \"Camera not ready\", description: \"The camera stream is not active. Please try again.\", variant: \"destructive\" });\n        enableCam();\n        return;\n    }\n\n    setVideoDataUri(null);\n    facialDataRef.current = [];\n    videoChunksRef.current = [];\n    \n    try {\n      mediaRecorderRef.current = new MediaRecorder(streamRef.current, { mimeType: 'video/webm' });\n      \n      mediaRecorderRef.current.ondataavailable = (event) => {\n          if (event.data && event.data.size > 0) {\n            videoChunksRef.current.push(event.data);\n          }\n      };\n\n      mediaRecorderRef.current.onstop = () => {\n          const videoBlob = new Blob(videoChunksRef.current, { type: 'video/webm' });\n          const reader = new FileReader();\n          reader.onloadend = () => {\n              const base64String = reader.result as string;\n              setVideoDataUri(base64String);\n              onRecordingComplete(base64String, facialDataRef.current);\n          };\n          reader.readAsDataURL(videoBlob);\n      };\n\n      mediaRecorderRef.current.start();\n      setIsRecording(true);\n    } catch (error) {\n       console.error(\"Error starting MediaRecorder:\", error);\n       toast({\n         title: \"Recording Error\",\n         description: \"Could not start video recording. Please ensure your browser supports WebM format and camera is not in use.\",\n         variant: \"destructive\"\n       });\n    }\n  };\n\n  const stopRecording = () => {\n    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {\n      mediaRecorderRef.current.stop();\n    }\n    setIsRecording(false);\n  };\n\n  return (\n    <div className=\"flex flex-col items-center space-y-4 p-4 border rounded-lg shadow-sm bg-muted/20\">\n      <div className=\"w-full aspect-video bg-black rounded-md overflow-hidden relative flex items-center justify-center\">\n        <video ref={videoRef} className=\"w-full h-full object-cover\" autoPlay muted playsInline />\n        {isInitializing && (\n            <div className=\"absolute inset-0 flex flex-col items-center justify-center text-white bg-black/50 p-4\">\n                <Loader2 className=\"h-8 w-8 animate-spin mb-2\" />\n                <p>Loading AI models...</p>\n            </div>\n        )}\n        {hasCameraPermission === false && (\n             <div className=\"absolute inset-0 flex flex-col items-center justify-center text-destructive-foreground bg-destructive/80 p-4 text-center\">\n                <VideoOff className=\"h-12 w-12 mb-4\" />\n                <h3 className=\"font-bold\">Camera Access Required</h3>\n                <p className=\"text-sm\">Please allow camera and microphone access.</p>\n                <Button onClick={enableCam} variant=\"secondary\" size=\"sm\" className=\"mt-4\">Retry Permissions</Button>\n            </div>\n        )}\n         {hasCameraPermission === null && !isInitializing && (\n             <div className=\"absolute inset-0 flex flex-col items-center justify-center text-muted-foreground\">\n                <Video className=\"h-12 w-12 mb-4 animate-pulse\" />\n                <p>Waiting for camera...</p>\n            </div>\n        )}\n      </div>\n\n      <Button\n        onClick={isRecording ? stopRecording : startRecording}\n        disabled={isProcessing || !hasCameraPermission || isInitializing}\n        variant={isRecording ? \"destructive\" : \"default\"}\n        size=\"lg\"\n        className=\"w-full\"\n      >\n        {isRecording ? (\n          <>\n            <StopCircle className=\"mr-2 h-5 w-5 animate-pulse\" />\n            Stop Recording\n          </>\n        ) : (\n          <>\n            <Mic className=\"mr-2 h-5 w-5\" />\n            {videoDataUri ? 'Record Again' : 'Start Recording Answer'}\n          </>\n        )}\n      </Button>\n\n      {isBrowserSupported === false && (\n          <p className='text-xs text-destructive'>Video recording is not supported in your browser.</p>\n      )}\n\n      {videoDataUri && !isRecording && !isProcessing && (\n        <div className=\"w-full mt-2\">\n          <p className=\"text-sm text-muted-foreground mb-1\">Your recorded answer:</p>\n          <video controls src={videoDataUri} className=\"w-full rounded-md\" />\n        </div>\n      )}\n    </div>\n  );\n};\n",
        "type": "registry:component",
        "target": "components/video-recorder.tsx"
      },
      {
        "path": "hooks/use-mobile.tsx",
        "content": "import * as React from \"react\"\n\nconst MOBILE_BREAKPOINT = 768\n\nexport function useIsMobile() {\n  const [isMobile, setIsMobile] = React.useState<boolean | undefined>(undefined)\n\n  React.useEffect(() => {\n    const mql = window.matchMedia(`(max-width: ${MOBILE_BREAKPOINT - 1}px)`)\n    const onChange = () => {\n      setIsMobile(window.innerWidth < MOBILE_BREAKPOINT)\n    }\n    mql.addEventListener(\"change\", onChange)\n    setIsMobile(window.innerWidth < MOBILE_BREAKPOINT)\n    return () => mql.removeEventListener(\"change\", onChange)\n  }, [])\n\n  return !!isMobile\n}\n",
        "type": "registry:lib",
        "target": "hooks/use-mobile.tsx"
      },
      {
        "path": "hooks/use-toast.ts",
        "content": "\"use client\"\n\n// Inspired by react-hot-toast library\nimport * as React from \"react\"\n\nimport type {\n  ToastActionElement,\n  ToastProps,\n} from \"@/components/ui/toast\"\n\nconst TOAST_LIMIT = 1\nconst TOAST_REMOVE_DELAY = 1000000\n\ntype ToasterToast = ToastProps & {\n  id: string\n  title?: React.ReactNode\n  description?: React.ReactNode\n  action?: ToastActionElement\n}\n\nconst actionTypes = {\n  ADD_TOAST: \"ADD_TOAST\",\n  UPDATE_TOAST: \"UPDATE_TOAST\",\n  DISMISS_TOAST: \"DISMISS_TOAST\",\n  REMOVE_TOAST: \"REMOVE_TOAST\",\n} as const\n\nlet count = 0\n\nfunction genId() {\n  count = (count + 1) % Number.MAX_SAFE_INTEGER\n  return count.toString()\n}\n\ntype ActionType = typeof actionTypes\n\ntype Action =\n  | {\n      type: ActionType[\"ADD_TOAST\"]\n      toast: ToasterToast\n    }\n  | {\n      type: ActionType[\"UPDATE_TOAST\"]\n      toast: Partial<ToasterToast>\n    }\n  | {\n      type: ActionType[\"DISMISS_TOAST\"]\n      toastId?: ToasterToast[\"id\"]\n    }\n  | {\n      type: ActionType[\"REMOVE_TOAST\"]\n      toastId?: ToasterToast[\"id\"]\n    }\n\ninterface State {\n  toasts: ToasterToast[]\n}\n\nconst toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>()\n\nconst addToRemoveQueue = (toastId: string) => {\n  if (toastTimeouts.has(toastId)) {\n    return\n  }\n\n  const timeout = setTimeout(() => {\n    toastTimeouts.delete(toastId)\n    dispatch({\n      type: \"REMOVE_TOAST\",\n      toastId: toastId,\n    })\n  }, TOAST_REMOVE_DELAY)\n\n  toastTimeouts.set(toastId, timeout)\n}\n\nexport const reducer = (state: State, action: Action): State => {\n  switch (action.type) {\n    case \"ADD_TOAST\":\n      return {\n        ...state,\n        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),\n      }\n\n    case \"UPDATE_TOAST\":\n      return {\n        ...state,\n        toasts: state.toasts.map((t) =>\n          t.id === action.toast.id ? { ...t, ...action.toast } : t\n        ),\n      }\n\n    case \"DISMISS_TOAST\": {\n      const { toastId } = action\n\n      // ! Side effects ! - This could be extracted into a dismissToast() action,\n      // but I'll keep it here for simplicity\n      if (toastId) {\n        addToRemoveQueue(toastId)\n      } else {\n        state.toasts.forEach((toast) => {\n          addToRemoveQueue(toast.id)\n        })\n      }\n\n      return {\n        ...state,\n        toasts: state.toasts.map((t) =>\n          t.id === toastId || toastId === undefined\n            ? {\n                ...t,\n                open: false,\n              }\n            : t\n        ),\n      }\n    }\n    case \"REMOVE_TOAST\":\n      if (action.toastId === undefined) {\n        return {\n          ...state,\n          toasts: [],\n        }\n      }\n      return {\n        ...state,\n        toasts: state.toasts.filter((t) => t.id !== action.toastId),\n      }\n  }\n}\n\nconst listeners: Array<(state: State) => void> = []\n\nlet memoryState: State = { toasts: [] }\n\nfunction dispatch(action: Action) {\n  memoryState = reducer(memoryState, action)\n  listeners.forEach((listener) => {\n    listener(memoryState)\n  })\n}\n\ntype Toast = Omit<ToasterToast, \"id\">\n\nfunction toast({ ...props }: Toast) {\n  const id = genId()\n\n  const update = (props: ToasterToast) =>\n    dispatch({\n      type: \"UPDATE_TOAST\",\n      toast: { ...props, id },\n    })\n  const dismiss = () => dispatch({ type: \"DISMISS_TOAST\", toastId: id })\n\n  dispatch({\n    type: \"ADD_TOAST\",\n    toast: {\n      ...props,\n      id,\n      open: true,\n      onOpenChange: (open) => {\n        if (!open) dismiss()\n      },\n    },\n  })\n\n  return {\n    id: id,\n    dismiss,\n    update,\n  }\n}\n\nfunction useToast() {\n  const [state, setState] = React.useState<State>(memoryState)\n\n  React.useEffect(() => {\n    listeners.push(setState)\n    return () => {\n      const index = listeners.indexOf(setState)\n      if (index > -1) {\n        listeners.splice(index, 1)\n      }\n    }\n  }, [state])\n\n  return {\n    ...state,\n    toast,\n    dismiss: (toastId?: string) => dispatch({ type: \"DISMISS_TOAST\", toastId }),\n  }\n}\n\nexport { useToast, toast }\n",
        "type": "registry:lib",
        "target": "hooks/use-toast.ts"
      },
      {
        "path": "lib/utils.ts",
        "content": "import { clsx, type ClassValue } from \"clsx\"\nimport { twMerge } from \"tailwind-merge\"\n\nexport function cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs))\n}\n",
        "type": "registry:lib",
        "target": "lib/utils.ts"
      }
    ]
  }